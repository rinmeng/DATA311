---
title: "Assignment 2"
subtitle: "DATA311 - Machine Learning"
author: "Rin Meng, 51940633"
toc: true
format:
  html:
    embed-resources: true
    df-print: paged
editor: visual
---

# Exploratory / Pre-Processing

### Exercise 1

a\. Classification\
d. Supervised

### Exercise 2 (Data Preparing and Summary)

```{r}
telco <- read.table(("telcochurn.csv"), header=TRUE, sep = ",")
telco <- na.omit(telco)
# Make sure its factored into levels Yes and No
telco$Churn <- as.factor(telco$Churn)
telco$tenure <- as.numeric(telco$tenure)
telco$gender <- as.factor(telco$gender)
telco$SeniorCitizen <- as.factor(telco$SeniorCitizen)
telco$Partner <- as.factor(telco$Partner)
telco$Dependents <- as.factor(telco$Dependents)
telco$PhoneService <- as.factor(telco$PhoneService)
telco$MultipleLines <- as.factor(telco$MultipleLines)
telco$InternetService <- as.factor(telco$InternetService)
telco$OnlineSecurity <- as.factor(telco$OnlineSecurity)
telco$OnlineBackup <- as.factor(telco$OnlineBackup)
telco$DeviceProtection <- as.factor(telco$DeviceProtection)
telco$TechSupport <- as.factor(telco$TechSupport)
telco$StreamingTV <- as.factor(telco$StreamingTV)
telco$StreamingMovies <- as.factor(telco$StreamingMovies)
telco$Contract <- as.factor(telco$Contract)
telco$PaperlessBilling <- as.factor(telco$PaperlessBilling)
telco$PaymentMethod <- as.factor(telco$PaymentMethod)
telco$MonthlyCharges <- as.numeric(telco$MonthlyCharges)
telco$TotalCharges <- as.numeric(telco$TotalCharges)
str(telco)
```

### Exercise 3 (Distribution of Churn Rates)

```{r}
library(ggplot2)

ggplot(telco, aes(x = Churn)) +
  geom_bar(fill = c("steelblue", "tomato")) +
  labs(title = "Customer Churn Frequency",
       x = "Churn Status",
       y = "Frequency") +
  theme_minimal()

```

Imbalanced class: The No's are marginally larger than the Yes's. This ultimately implies that our prediction can be biased and favors the majority side because the model may not learn enough on the minority side.

### Exercise 4 (Data Visualization for Categorical Variables)

```{r}



churnPlot <- function(predictor) {
  ggplot(telco, aes_string(x = predictor, fill = "Churn")) +  # Use aes_string for dynamic x aesthetic
    geom_bar(position = "fill") +
    labs(title = paste("Churn Rate by", predictor), x = predictor, y = "Proportion") +  # Make title dynamic
    theme_minimal()
}

# Identify categorical predictors
predictors <- colnames(telco)
exclude_columns <- c("customerID","Churn", "MonthlyCharges", "TotalCharges","tenure","tenure_bins","TotalCharges_bins","MonthlyCharges_bin")
predictors <- predictors[!predictors %in% exclude_columns]


for (predictor in predictors) {
  print(churnPlot(predictor))
}


```

Based on these plots, we can see that the categorical predictor that seems to be associated churn would be; SeniorCitizen, Partner, Dependents, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, Contract, PaperlessBilling, PaymentMethod, MonthlyCharges, TotalCharges.

### Exercise 5 (Data Visualization for Continuous Variables)

```{r}

# Create binned tenure categories 
telco$tenure_bins <- cut(telco$tenure, 
                         breaks = c(0, 12, 24, 36, 48, 60, Inf), 
                         labels = c("0-12", "13-24", "25-36", "37-48", "49-60", "60+"),
                         right = FALSE,   # Include the lower bound, exclude the upper
                         include.lowest = TRUE)  # Include the lowest value
ggplot(telco, aes(x = tenure_bins, fill = Churn)) +
    geom_bar(position = "fill") +
    labs(title = "Churn Rate by tenure Bins", x = "Tenure (Months)", y = "Proportion") +
    theme_minimal()


telco$MonthlyCharges_bins <- cut(telco$MonthlyCharges, 
                         breaks = c(0, 20, 40, 60, 80, 100, Inf), 
                         labels = c("0-20", "21-40", "41-60", "61-80", "81-100", "100+"),
                         right = FALSE,   # Include the lower bound, exclude the upper
                         include.lowest = TRUE)  # Include the lowest value
ggplot(telco, aes(x = MonthlyCharges_bins, fill = Churn)) +
    geom_bar(position = "fill") +
    labs(title = "Churn Rate by MonthlyCharges Bins", x = "MonthlyCharges ($)", y = "Proportion") +
    theme_minimal()

telco$TotalCharges_bins <- cut(telco$TotalCharges, 
                         breaks = c(0, 800, 1600, 2400, 3200, 4000, 4800, Inf), 
                         labels = c("0-800", "801-1600", "1601-2400", "2401-3200", "3201-4000", "4001-4800", "4800+"),
                         right = FALSE,   # Include the lower bound, exclude the upper
                         include.lowest = TRUE)  # Include the lowest value
ggplot(telco, aes(x = TotalCharges_bins, fill = Churn)) +
    geom_bar(position = "fill") +
    labs(title = "Churn Rate by TotalCharges Bins", x = "TotalCharges ($)", y = "Proportion") +
    theme_minimal()

```

Based on these plots, the continuous variable that seems to be associated with the customer churn would be; tenure, MontlyCharges, and TotalCharges.

### Exercise 6 (Data Splitting)

```{r}
set.seed(51940633)
# First split: 70% training and 30% test
sampleSize <- floor(0.7 * nrow(telco))
set <- sample(seq_len(nrow(telco)), size = sampleSize)

telcoTrain <- telco[set, ]
telcoTest <- telco[-set, ]

paste("telcoTrain has", nrow(telcoTrain), "rows which is", sprintf("%.2f",100 * nrow(telcoTrain)/nrow(telco)), "% of the dataset")
paste("telcoTest has", nrow(telcoTest), "rows which is", sprintf("%.2f",100 * nrow(telcoTest)/nrow(telco)), "% of the dataset")

```

# Model Fitting

### Exercise 7 (Logistic Regression)

```{r}

set.seed(51940633)
telcoTrain.glm <- glm(Churn ~ tenure + MonthlyCharges + TotalCharges, data=telcoTrain, family= binomial)

summary(telcoTrain.glm)
```

### Exercise 8 (Interpret coefficients)

1.  tenure: -0.0645466
    -   Interpretation: For each added month of tenure, the odds of customer churning decreases by 0.0645466. So that means that if the customer stays with the company longer, the likelihood of them churning decreases. Also implying that customer might feel like they don't want to churn because they are satisfied with their offers/plans.
2.  MonthlyCharges: 0.0305762
    -   Interpretation: For each additional dollar that the company charges per month, the odds of customer churning increases by 0.0305762. So that means if the customer gets charged more per month, they are more likely to churn. Also implying that customer may be unsatisfied with the given offers/plans from the company.
3.  TotalCharges: 0.0001249
    -   Interpretation: For each extra dollar that the company makes from the customer, the odds of customer churning is 0.0001249 more. So that means if the customer's total charge increases, the more likely that the customer will churn. This predictor isn't as significant as the other two, but it can suggest that the customer that spends more total money on the company are more likely to churn.

### Exercise 9 (Your Logistic Regression)

```{r}
set.seed(51940633)
# Now we use those predictors that is significant
telcoTrain.glm9 <- glm(Churn ~ SeniorCitizen + tenure * Contract +
                          PaymentMethod + PaperlessBilling + 
                         OnlineSecurity + TechSupport + StreamingMovies + 
                         InternetService + MonthlyCharges + TotalCharges,
                       data=telcoTrain, family=binomial)
summary(telcoTrain.glm9)
```

### Exercise 10 (K-Nearest Neighbors Classification)

```{r}
library(caret)
library(dplyr)
set.seed(51940633)

# Define the training control for cross-validation
control_10fold <- trainControl(method = "cv", number = 10)

# Train KNN model
# Specify a grid of k values (for example, from 1 to 50)
k_values <- data.frame(k = seq(1, 50, by = 1)) 

knn_model <- train(Churn ~ SeniorCitizen + tenure + Contract +
                          PaymentMethod + PaperlessBilling + 
                         OnlineSecurity + TechSupport + StreamingMovies + 
                         InternetService + MonthlyCharges + TotalCharges,
                       data=telcoTrain,
                   method = "knn",
                   trControl = control_10fold,
                   tuneGrid = k_values)

# Display the results
print(knn_model)
plot(knn_model)
```

As per the accuracy value above, the optimal model uses k = `r knn_model$bestTune$k`.

### Exercise 11 (Retrain KNN)

```{r}
library(caret)
library(dplyr)
set.seed(51940633)
control_10fold <- trainControl(method = "cv", number = 10)

optimal_k <- data.frame(k = knn_model$bestTune$k)

knn_model_optimal <- train(Churn ~ SeniorCitizen + tenure + Contract +
                                 PaymentMethod + PaperlessBilling + 
                                 OnlineSecurity + TechSupport + StreamingMovies + 
                                 InternetService + MonthlyCharges + TotalCharges,
                          data = telcoTrain,
                          method = "knn",
                          trControl = control_10fold,
                          tuneGrid = optimal_k)

print(knn_model_optimal)

```

### Exercise 12 (Seeds for KNN)

The non-deterministic of the algorithm is that it that when we apply techniques like k-fold cross validation, the data is randomly split. Which means the data and outcome will vary slightly each run due to the randomness, which can lead to different performance in accuracy.

### Exercise 13 (Linear Discriminant Analysis (LDA))

```{r}
library(MASS)

lda_model <- lda(Churn ~ tenure + TotalCharges, data = telcoTrain)
print(lda_model)
```

### Exercise 14 (Quadratic Discriminant Analysis)

```{r}
library(MASS)

qda_model <- qda(Churn ~ tenure + TotalCharges, data = telcoTrain)
print(qda_model)
```

### Exercise 15 (Confusion Matrices)

```{r}
# Exercise 7
logistic_pred_q7 <- predict(telcoTrain.glm, newdata = telcoTest, type = "response")
logistic_pred_class_q7 <- ifelse(logistic_pred_q7 > 0.5, "Yes", "No")
logistic_conf_matrix_q7 <- confusionMatrix(factor(logistic_pred_class_q7), telcoTest$Churn)
print(logistic_conf_matrix_q7$table)

# Exercise 9
logistic_pred_q9 <- predict(telcoTrain.glm9, newdata = telcoTest, type = "response")
logistic_pred_class_q9 <- ifelse(logistic_pred_q9 > 0.5, "Yes", "No")
logistic_conf_matrix_q9 <- confusionMatrix(factor(logistic_pred_class_q9), telcoTest$Churn)
print(logistic_conf_matrix_q9$table)

# Exercise 11
knn_pred_optimal <- predict(knn_model_optimal, newdata = telcoTest)
knn_conf_matrix_optimal <- confusionMatrix(knn_pred_optimal, telcoTest$Churn)
print(knn_conf_matrix_optimal$table)

# Exercise 13
lda_pred <- predict(lda_model, newdata = telcoTest)
lda_conf_matrix <- confusionMatrix(lda_pred$class, telcoTest$Churn)
print(lda_conf_matrix$table)

# Exercise 14
qda_pred <- predict(qda_model, newdata = telcoTest)
qda_conf_matrix <- confusionMatrix(qda_pred$class, telcoTest$Churn)
print(qda_conf_matrix$table)
```

### Exercise 16 (Model Comparison)

The recall/true positive rate formula is given by

$$
\text{Recall} = \frac{\text{True Positive}}{\text{True Positive} + \text{False Negatives}}
$$

$$
 = \frac{\text{Prediction: Yes, Reference: Yes}}{(\text{Prediction: Yes, Reference: Yes}) + (\text{Prediction: No, Reference: Yes})}
$$\

| Model                                    | Recall            |
|------------------------------------------|-------------------|
| Basic Logistic Regression, Exercise 7    | `r 254/(254+307)` |
| My Logistic Regression Model, Exercise 9 | `r 311/(311+250)` |
| Retrained KNN Model, Exercise 11         | `r 247/(247+314)` |
| LDA, Exercise 13                         | `r 151/(151+410)` |
| QDA, Exercise 14                         | `r 420/(420+141)` |

Overall, we can clearly see that the QDA model is the winner here, giving us `r round(420/(420+141)*100, 2)`% accuracy of customer who churned.

### Exercise 17 (Bootstrapping for Uncertainty)

```{r}
# Load necessary libraries
library(boot)
library(MASS)  # For QDA model

# Create a function to calculate the statistic of interest
recall_stats <- function(data, indices) {
  sample_data <- data[indices, ]  # Create bootstrap sample
  
  # Fit QDA model using the formula above
  fit <- qda(Churn ~ tenure + TotalCharges, data = sample_data)
  
  # Predict on the resampled data
  predictions <- predict(fit, sample_data)$class
  
  # Calculate recall
  true_positives <- sum(predictions == "Yes" & sample_data$Churn == "Yes")
  total_actual_positives <- sum(sample_data$Churn == "Yes")
  recall <- true_positives / total_actual_positives
  
  return(recall)
}

# Number of bootstrap samples
B <- 1000

# Set a seed for reproducibility
set.seed(6663492)

# Perform bootstrapping
boot_recall_results <- boot(data = telcoTrain, statistic = recall_stats, R = B)

# View the results
print(boot_recall_results)

# Calculate the 95% confidence interval
ci_95 <- boot.ci(boot_recall_results, type = "perc")
print(ci_95)

```

The original value at 0.7148318 implies that our QDA model correctly identifies 71.5% of all customer who churned, and the bias of -0.0012 implies that our bootstrapping recall is very close towards the original recall.

Our 95% confidence interval tells us that, our accuracy of identifying customer who churns can very between 68% and 74% with a different data.
