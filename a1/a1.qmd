---
title: "Assignment 1"
subtitle: "DATA311 - Machine Learning"
author: "Rin Meng, 51940633"
toc: true
format:
  html:
    embed-resources: true
    df-print: paged
editor: visual
---

## Exploratory / Pre-processing

### Exercise 1

b\. Regression

c\. Supervised

### Exercise 2

```{r}

abalone <- read.table(("abalone.data"), header = FALSE, sep = ",")
colnames(abalone) <- c("Sex", "Length", "Diameter", "Height", "Whole_Weight", "Shucked_Weight", "Viscera_Weight", "Shell_Weight", "Rings")

str(abalone)

```

### Exercise 3

```{r}

hist(abalone$Rings, xlab = "Abalone age",breaks=30 , ylab = "Abalone count", main = "Histogram of abalone count vs abalone age")
```

We see that this is an observation of a skewed normally distributed histogram. This should not break any assumptions of the linear model because in a linear model, we should have a data that follows **Linearity, Independence, Homoscedasticity, Normality** and from this observation alone, we do not have enough factors to point out if it breaks any of the assumptions, there may be a few outliers that made the graph be skewed.

### Exercise 4

```{r}

set.seed(51940633)

# count half of the rows
sampleSize <- floor(0.5 * nrow(abalone))

# split rows into 2 sets
set <- sample(seq_len(nrow(abalone)), size = sampleSize)

abaloneTrain <- abalone[set, ]

abaloneTest <- abalone[-set, ]

abaloneTrain
abaloneTest

```

## Simple Linear Regression

### Exercise 5

```{r}
abaloneTrain.lm <- lm(Rings ~ Length, data=abaloneTrain)
summary(abaloneTrain.lm)
```

Since the p value for the **length** coefficient is incredibly small at $2 \times 10^{-16}$, we can safely say that there is a significant relationship between abalone's **length** and **rings**.

### Exercise 6

The **coefficient of determination**, $R^2$ , also known as the **Multiple R-squared** value, is at $0.2866$. This therefore, means that about $28.66%$ of the presented abalone's **ring** is explainable by the abalone's **length**.

### Exercise 7

```{r}
plot(abaloneTrain$Length, abaloneTrain$Rings, xlab="Abalone length (mm)", ylab="Abalone rings",
     pch=16,cex=0.3, main = "Scatter plot of rings vs length")
abline(abaloneTrain.lm, col = "red")

```

### Exercise 8

The **scatter plot** of **Rings vs Length** indicates that there is an underlying pattern, therefore it implies that this model violated:

1.  **Homoscedasticity:** There is no visible constant variance between each dots, therefore, this model is heteroscedasticity.

2.  **Linearity:** Since there is a pattern, therefore, linearity is also violated.

### Exercise 9

```{r}
plot(abaloneTrain.lm, which = 1, pch=16, cex=0.3)
```

**Residuals vs fitted** plot shows that there are patterns in the plot, which suggests that the relationship between the predictor and response variable is not purely linear and this suggests that the model is giving a biased estimate.

## **Multiple Linear Regression**

### Exercise 10

```{r}
abaloneTrain.lm2 <- lm(Rings ~ Length + I(Length^2), data=abaloneTrain)
summary(abaloneTrain.lm2)


plot(abaloneTrain.lm2, pch=16, cex=0.3)

```

1.  **P-value:** The p-value of the polynomial term, `I(Length^2)` equates to $6.01 \times 10^{-7}$ which is much less than $0.05$ so it is still statistically significant.
2.  **Adjusted R-squared: T**he adjusted R-squared value has increased from $0.2866$ in the linear model to $0.2947$ in the polynomial model, indicating a better fit with the inclusion of the quadratic term.
3.  **Residual Standard Error**: The residual standard error has slightly decreased from $2.677$ in the linear model to $0.2947$ in the polynomial model, indicating a better fit.
4.  **Violations:**
    -   **Linearity violated:** the **Residuals vs Fitted** plot shows us a very pattern-heavy plot, making it non-linear, as we look for random scatters around the red-line.
    -   **Normality violated:** the **QQ Residuals** plot shows us that the residual points does not lie on the dashed-line, making it not normally distributed,
    -   **Homoscedasticity violated:** the **Scale-Location** plot is showing us patterns with no constant variance.
    -   **Independence not violated:** the **Residuals vs Leverage** plot is showing us that there are no points that clearly lies outside of **Cook's distance**, so it does not have disproportionate impact on the model.

### Exercise 11

```{r}
abaloneTrain.lm3 <- lm(Rings ~ Length + Sex, data=abaloneTrain)
summary(abaloneTrain.lm3)

```

By adding the `Sex` as a categorical predictor in the regression model, we can clearly that:

-   Sex = I is highly significant with a p-value of $2 \times 10^{-16}$.

-   Sex = M is insignificant with a p-value of $0.348$.

-   **Model with only Length:** Adjusted R-squared = $0.2866$

-   **Model with Length + Sex:** Adjusted R-squared = $0.3152$

The adjusted R-squared value increased from $0.2866$ to $0.3152$, indicating that adding `Sex` as a predictor improves the model fit by explaining more variance in the response variable.

Therefore, we can conclude that, adding the `Sex` as a categorical predictor variable significantly improves the model.

### Exercise 12

```{=tex}
\begin{equation}
Rings = 
\begin{cases} 
  \hat{Y} = 4.4106 + 11.5287 \cdot Length & \text{if Female} \\
  \hat{Y} = 4.4106 + 11.5287 \cdot Length - 0.1314 & \text{if Male} \\
  \hat{Y} = 4.4106 + 11.5287 \cdot Length - 1.4391 & \text{if Infant}
\end{cases}
\end{equation}
```
### Exercise 13

The reference level for the model fitted in Exercise 11 is **Female** because the coefficients for both, `Sex = "M"` and `Sex = "I"` are provided, therefore, their values are interpreted with relative to the reference category, `Sex = "F"`.

### Exercise 14

Based on this data, and the predicted $\hat{Y}$ shown above, it seems to be that the **female** abalone tend to have the oldest average age, then **male**, then **infant**.

### Exercise 15

```{r}

abaloneTrain.lm3withWholeWeight <- lm(Rings ~ Length + Sex + Whole_Weight, data=abaloneTrain)

abaloneTrain.forward <- step(abaloneTrain.lm3, scope = list(lower = abaloneTrain.lm3, upper = abaloneTrain.lm3withWholeWeight), direction = "forward")

summary(abaloneTrain.forward)

```

## **Multiple Linear Regression**

### Exercise 16

```{r}
numeric_columns <- sapply(abalone, is.numeric)

corTable <- cor(abalone[, numeric_columns])
diag(corTable) <- NA

maxCor <- max(corTable, na.rm = TRUE)

maxCorIndex <- which(corTable == maxCor, arr.ind = TRUE)

corTable
maxCor
maxCorIndex
```

Ignoring correlation between the predictor and itself, the highest correlation we have is the two variables, `Diameter` and `Length` which is the variable **Length** and **Diameter**, both being highly correlated to each other out of the other pairs.

Implications of this **multicollinearity** is:

-   **Unstable coefficient estimates**: the coefficients of the correlated predictors can become very sensitive to small changes in the model that we have. Small changes can lead to very skewed outcome.

-   **Inflated standard errors**: high correlation between predictors can inflate standard errors of coefficient estimates.

-   **Reduction of overall interpretability**: it will be difficult to determine the effect of each predictor in the response variable. High correlation means it is providing redundant information.

### Exercise 17

```{r}
abaloneTrain.lm4 <- lm(Rings ~ Length + Whole_Weight, data=abaloneTrain)
summary(abaloneTrain.lm4)
```

Comparing the **R-squared** and **Adjusted R-squared** values for this model with **Exercise 5**, we see that in **Exercise 5**, we have a linear model of **Rings** as a response variable and **Length** as the predictor variable:

-   **Multiple R-squared**: $0.2869$

-   **Adjusted R-squared**: $0.2866$

Here, when we have a linear model of **Rings** as a response variable and **Length** and **Whole Weight** as the predictor variable:

-   **Multiple R-squared**: $0.2907$

-   **Adjusted R-squared**: $0.2901$

What do these values tell us: The **Multiple R-squared** value increased from $0.2869$ to $0.2907$. This indicates that the addition of **Whole Weight** as a predictor has just slightly improved the model. The **Adjusted R-squared** value also increased from $0.2866$ to $0.2901$. This suggests that the improvement in the modelâ€™s performance just because we added new predictors, but the **Whole Weight** variable actually contributes information to our model.

Why the difference: **Multiple R-squared** will always be increased when there are new predictor variable introduced even if the new predictors doesn't improve the model, and that is why **Adjusted R-squared** will provide a more accurate measurement of the model performance by taking away the contribution of unnecessary predictors.

### Exercise 18

```{r}
interactionModel <- lm(Rings ~ Length * Whole_Weight, data = abaloneTrain)
summary(interactionModel)
```

The **interaction term** `Length:Whole_Weight` has a very low p-value at $2 \times 10^{-16}$, making it statistically significant. The negative coefficient of the interaction term suggests that, as the variable **Whole Weight** increases, the effect of **Length** on the number of **Rings** decreases.

### Exercise 19

```{r}
multipleRegression <- lm (Rings ~ ., data = abaloneTrain)
summary(multipleRegression)
```

The predictors that are statistically significant (p-value $< 0.05$) are:

1.  **Sex = I**: p-value $= 8.67 \times 10^{-12}$

2.  **Diameter**: p-value $= 0.0145$

3.  **Height**: p-value $= 1.04 \times 10^{-05}$

4.  **Whole_Weight**: p-value = $2 \times 10^{-16}$

5.  **Shucked_Weight**: p-value $< 2 \times 10^{-16}$

6.  **Viscera_Weight**: p-value $= 7.62 \times 10^{-10}$

7.  **Shell_Weight**: p-value $= 1.85 \times 10^{-07}$

### Exercise 20

I would have chose:

1.  **Sex = Male**: The p-value is 0.9309 which is not statistically significant, therefore it is unlikely to have impact on the model's performance.

2.  **Length**: The p-value is 0.2859, which is also not statistically significant, and also, it is highly correlated with the **Diameter**.

I would expect the model's prediction to be better, where the **Adjusted R-squared** value is much higher of any of the observed model that we have created throughout this exercise.

### Exercise 21

```{r}
myModel <- lm(Rings ~ Sex * Diameter * Height * Whole_Weight * 
            Shucked_Weight * Viscera_Weight * Shell_Weight, data = abaloneTrain)
summary(myModel)$adj.r.squared


```

I chose this model because I excluded the variable that were not statistically significant, such as the variable **Length** and **Sex = M**.

This model performed a little bit better than the all of the other models because the **Adjusted R-squared** is up.

## Model Comparison

### Exercise 22

| Exercise | Adjusted $R^2$                                             | $RSE_{train}$                                      | $MSE_{test}$                                                                                      |
|----------|------------------------------------------------------------|----------------------------------------------------|---------------------------------------------------------------------------------------------------|
| 5        | `r summary(abaloneTrain.lm)$adj.r.squared`                 | `r summary(abaloneTrain.lm)$sigma`                 | `r mean((abaloneTest$Rings - predict(abaloneTrain.lm, newdata = abaloneTest))^2)`                 |
| 10       | `r summary(abaloneTrain.lm2)$adj.r.squared`                | `r summary(abaloneTrain.lm2)$sigma`                | `r mean((abaloneTest$Rings - predict(abaloneTrain.lm2, newdata = abaloneTest))^2)`                |
| 11       | `r summary(abaloneTrain.lm3)$adj.r.squared`                | `r summary(abaloneTrain.lm3)$sigma`                | `r mean((abaloneTest$Rings - predict(abaloneTrain.lm3, newdata = abaloneTest))^2)`                |
| 15       | `r summary(abaloneTrain.lm3withWholeWeight)$adj.r.squared` | `r summary(abaloneTrain.lm3withWholeWeight)$sigma` | `r mean((abaloneTest$Rings - predict(abaloneTrain.lm3withWholeWeight, newdata = abaloneTest))^2)` |
| 17       | `r summary(abaloneTrain.lm4)$adj.r.squared`                | `r summary(abaloneTrain.lm4)$sigma`                | `r mean((abaloneTest$Rings - predict(abaloneTrain.lm4, newdata = abaloneTest))^2)`                |
| 18       | `r summary(interactionModel)$adj.r.squared`                | `r summary(interactionModel)$sigma`                | `r mean((abaloneTest$Rings - predict(interactionModel, newdata = abaloneTest))^2)`                |
| 19       | `r summary(multipleRegression)$adj.r.squared`              | `r summary(multipleRegression)$sigma`              | `r mean((abaloneTest$Rings - predict(multipleRegression, newdata = abaloneTest))^2)`              |
| 21       | `r summary(myModel)$adj.r.squared`                         | `r summary(myModel)$sigma`                         | `r mean((abaloneTest$Rings - predict(myModel, newdata = abaloneTest))^2)`                         |

### Exercise 23

For the best model to be picked, we have to follow these criteria:

1.  **Adjusted** $R^2$ **:** Higher values indicate a better fit.

2.  **RSE test:** Lower values indicate a better fit.

3.  **MSE test:** Lower values indicate a better fit.

Therefore, our best models listed from above should be the one from **Exercise 19** and **Exercise 21** where all of the criteria are very similar and is the closest we can get to our criteria out of all of the other models.

## KNN Regression

```{r}

library(caret)
knn_mse_test = NULL

for (k in c(10, 50, 100, 200, 300)){
  kmod <- knnreg(Rings ~ Length, data = abaloneTrain, k = k)
  yhat = predict(kmod, abaloneTest)
  knn_mse_test[k] = mean((abaloneTest$Length - yhat)^2)
}

plot(knn_mse_test, type = "b", ylab = "MSE test", xlab = "k", main = "KNN regression")
```

### Exercise 24

-   **k = 10,** The MSE is relatively high, suggesting that it is over-fitting the data, therefore, high variance, low bias.

-   **k = 50,** The model has a very low MSE test value, lower than all the other k indices, making it the optimal balance between bias and variance. Also we see MSE decreasing as well from **k = 10** which implies that increasing the **k** helps reduce variance and improve the model's performance.

-   **k = 100,** The MSE starts to increase, indicating that the model is starting to under-fitting as **k** increases.

-   **k = 200,** The MSE is at its highest, indicating that model has a very low variance but very bias to the training set.

-   **k = 300,** The MSE starts to fall off again, but it is still higher than **k = 50**.

Now we know, visually, that the optimal model lies somewhere around **k = 50** where it has a balance between variance and bias.

### Exercise 25

The model is **over-fitting** the data at **k = 10**. Because when **k** is low, the model is highly sensitive to the training data. It basically memorizes the training examples, capturing the noise in the data and the pattern as well. This results in high variance, where the model performs well on the training data but very poorly on new unseen data, leading to a high test MSE.

### Exercise 26

Based on the observed MSE values, I would pick **k = 50,** where the model is at its most optimal state and provides the best bias-variance trade off because it has the lowest MSE test value.
